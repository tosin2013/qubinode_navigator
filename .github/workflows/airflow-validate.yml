# =============================================================================
# Airflow Infrastructure DAG Validation
# =============================================================================
# Validates Airflow configuration, DAG syntax, and container builds
# Note: Full VM testing requires self-hosted runner with KVM access
#
# Jobs (all run on GitHub-hosted runners):
#
# 1. validate-dags:
#    - DAG Python syntax
#    - DAG linting (ADR-0045/ADR-0046 compliance)
#    - Airflow DAG imports
#
# 2. validate-containers:
#    - docker-compose.yml syntax
#    - Container image builds
#    - Required files verification
#
# 3. smoke-test (Full Integration):
#    - Spins up PostgreSQL + Marquez services
#    - Executes smoke_test_dag with OpenLineage
#    - Validates lineage data captured in Marquez
#    - Tests failure handling path
#    - Validates infrastructure DAG structures
#    - Tests MCP server startup
#
# What requires self-hosted runner with KVM:
# - VM provisioning (kcli, libvirt)
# - Full infrastructure DAG execution
# - Ansible playbook execution

name: Airflow Validation

on:
  push:
    branches: [main, develop]
    paths:
      - 'airflow/**'
      - '.github/workflows/airflow-validate.yml'
  pull_request:
    branches: [main]
    paths:
      - 'airflow/**'
  workflow_dispatch:

jobs:
  # ==========================================================================
  # DAG Validation (runs on GitHub-hosted runners)
  # ==========================================================================
  validate-dags:
    name: Validate DAGs
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install apache-airflow==2.10.4
          pip install apache-airflow-providers-postgres
          pip install pyyaml
          # Add Python bin directory to PATH for airflow CLI
          # With actions/setup-python, binaries go to the Python prefix bin dir
          PYTHON_BIN_DIR="$(python3 -c 'import sys; print(sys.prefix)')/bin"
          echo "Adding $PYTHON_BIN_DIR to PATH"
          echo "$PYTHON_BIN_DIR" >> $GITHUB_PATH
          # Verify airflow location
          ls -la "$PYTHON_BIN_DIR/airflow" || echo "airflow not found in $PYTHON_BIN_DIR"

      - name: Validate DAG syntax
        run: |
          echo "Validating DAG Python syntax..."
          for dag in airflow/dags/*.py; do
            echo "Checking $dag..."
            python3 -c "import ast; ast.parse(open('$dag').read())" || exit 1
          done
          echo "[OK] All DAGs have valid Python syntax"

      - name: Lint DAGs (ADR-0045/ADR-0046)
        run: |
          chmod +x airflow/scripts/lint-dags.sh
          ./airflow/scripts/lint-dags.sh airflow/dags/

      - name: Check Airflow DAG imports
        run: |
          cd airflow
          export AIRFLOW_HOME=$(pwd)
          export AIRFLOW__CORE__DAGS_FOLDER=$(pwd)/dags
          export AIRFLOW__CORE__LOAD_EXAMPLES=false
          export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:///airflow.db

          # Verify airflow is accessible
          echo "Airflow location: $(which airflow || echo 'not in PATH')"
          echo "Python location: $(which python3)"
          airflow version || { echo "[ERROR] airflow CLI not found"; exit 1; }

          # Initialize minimal Airflow DB
          airflow db init

          # List DAGs to check for import errors
          airflow dags list 2>&1 | tee dag_list.txt

          # Check for import errors (exclude "No data found" which is normal)
          if grep -i "error" dag_list.txt | grep -v "No data found"; then
            echo "[ERROR] DAG import errors found"
            exit 1
          fi
          echo "[OK] All DAGs imported successfully"

  # ==========================================================================
  # Container Build Validation (runs on GitHub-hosted runners)
  # ==========================================================================
  validate-containers:
    name: Validate Container Build
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Validate docker-compose.yml syntax
        run: |
          # Install docker-compose if needed
          docker compose version || pip install docker-compose

          cd airflow
          docker compose config --quiet && echo "[OK] docker-compose.yml is valid"

      - name: Build Airflow image
        run: |
          cd airflow
          docker compose build --no-cache airflow-webserver
          echo "[OK] Airflow image built successfully"

      - name: Verify required files exist
        run: |
          echo "Checking required files..."

          required_files=(
            "airflow/docker-compose.yml"
            "airflow/Dockerfile"
            "airflow/Makefile"
            "airflow/config/marquez.yml"
            "airflow/scripts/mcp_server_fastmcp.py"
            "airflow/scripts/clear-dag-cache.sh"
            "airflow/scripts/init-prerequisites.sh"
            "airflow/scripts/lint-dags.sh"
            "airflow/init-scripts/001-pgvector-schema.sql"
            "airflow/init-scripts/002-marquez-db.sql"
            "airflow/dags/smoke_test_dag.py"
          )

          for file in "${required_files[@]}"; do
            if [[ -f "$file" ]]; then
              echo "[OK] $file"
            else
              echo "[ERROR] Missing: $file"
              exit 1
            fi
          done

          echo "[OK] All required files exist"

  # ==========================================================================
  # Smoke Tests (runs on GitHub-hosted runners)
  # ==========================================================================
  # Tests actual DAG execution with OpenLineage/Marquez integration
  smoke-test:
    name: Smoke Test (Full Integration)
    runs-on: ubuntu-latest
    needs: [validate-dags, validate-containers]

    services:
      # PostgreSQL for Airflow metadata
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      # Marquez API for lineage tracking
      marquez:
        image: marquezproject/marquez:latest
        env:
          MARQUEZ_PORT: 5001
          MARQUEZ_ADMIN_PORT: 5002
          POSTGRES_HOST: postgres
          POSTGRES_PORT: 5432
          POSTGRES_DB: airflow
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
        ports:
          - 5001:5001
          - 5002:5002
        options: >-
          --health-cmd "curl -f http://localhost:5001/api/v1/namespaces || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          --health-start-period 30s

      # Marquez Web UI
      marquez-web:
        image: marquezproject/marquez-web:latest
        env:
          MARQUEZ_HOST: marquez
          MARQUEZ_PORT: 5001
        ports:
          - 3000:3000

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install apache-airflow==2.10.4
          pip install apache-airflow-providers-postgres
          pip install openlineage-airflow
          pip install pyyaml
          pip install httpx  # For MCP server testing
          # Add Python bin directory to PATH for airflow CLI
          # With actions/setup-python, binaries go to the Python prefix bin dir
          PYTHON_BIN_DIR="$(python3 -c 'import sys; print(sys.prefix)')/bin"
          echo "Adding $PYTHON_BIN_DIR to PATH"
          echo "$PYTHON_BIN_DIR" >> $GITHUB_PATH
          # Verify airflow location
          ls -la "$PYTHON_BIN_DIR/airflow" || echo "airflow not found in $PYTHON_BIN_DIR"

      - name: Initialize Airflow DB
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/airflow/dags
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
          OPENLINEAGE_URL: http://localhost:5001
          OPENLINEAGE_NAMESPACE: smoke_test
        run: |
          echo "Airflow location: $(which airflow || echo 'not in PATH')"
          airflow db init
          airflow users create \
            --username admin \
            --password admin \
            --firstname Admin \
            --lastname User \
            --role Admin \
            --email admin@example.com
          echo "[OK] Airflow DB initialized"

      - name: Wait for Marquez to be ready
        run: |
          echo "Waiting for Marquez API..."
          for i in {1..30}; do
            if curl -sf http://localhost:5001/api/v1/namespaces > /dev/null 2>&1; then
              echo "[OK] Marquez API is ready"
              break
            fi
            echo "  Attempt $i/30: Marquez not ready yet..."
            sleep 5
          done

          # Verify Marquez is responding
          curl -s http://localhost:5001/api/v1/namespaces | head -100
          echo ""

      - name: List and validate DAGs
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/airflow/dags
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
        run: |
          echo "========================================"
          echo "DAG Validation"
          echo "========================================"

          # List all DAGs
          airflow dags list

          # Check for import errors
          airflow dags list 2>&1 | tee dag_list.txt
          if grep -i "error" dag_list.txt | grep -v "No data found"; then
            echo "[ERROR] DAG import errors detected"
            exit 1
          fi

          echo ""
          echo "[OK] All DAGs loaded successfully"

      - name: Execute smoke_test_dag with OpenLineage
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/airflow/dags
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
          OPENLINEAGE_URL: http://localhost:5001
          OPENLINEAGE_NAMESPACE: smoke_test
          OPENLINEAGE_DISABLED: 'false'
        run: |
          echo "========================================"
          echo "Executing Smoke Test DAG"
          echo "========================================"

          # Run the smoke test DAG - this will emit OpenLineage events
          airflow dags test smoke_test_dag 2025-01-01 2>&1 | tee dag_execution.log

          # Check for success
          if grep -q "\[OK\] Smoke test completed successfully" dag_execution.log; then
            echo ""
            echo "[OK] Smoke test DAG executed successfully"
          else
            echo ""
            echo "[ERROR] Smoke test DAG failed"
            exit 1
          fi

      - name: Validate OpenLineage events in Marquez
        run: |
          echo "========================================"
          echo "Validating OpenLineage/Marquez Integration"
          echo "========================================"

          # Give Marquez time to process events
          sleep 5

          # Check namespaces
          echo "Checking Marquez namespaces..."
          NAMESPACES=$(curl -sf http://localhost:5001/api/v1/namespaces)
          echo "$NAMESPACES" | python3 -m json.tool || echo "$NAMESPACES"

          # Check for jobs in the smoke_test namespace
          echo ""
          echo "Checking for jobs..."
          JOBS=$(curl -sf "http://localhost:5001/api/v1/namespaces/smoke_test/jobs" 2>/dev/null || echo '{"jobs":[]}')
          echo "$JOBS" | python3 -m json.tool 2>/dev/null || echo "$JOBS"

          # Check for any runs
          echo ""
          echo "Checking for runs..."
          RUNS=$(curl -sf "http://localhost:5001/api/v1/jobs/runs" 2>/dev/null || echo '{"runs":[]}')
          echo "$RUNS" | python3 -m json.tool 2>/dev/null || echo "$RUNS"

          # Verify we have some data (OpenLineage integration working)
          if echo "$NAMESPACES" | grep -q "smoke_test\|default"; then
            echo ""
            echo "[OK] Marquez has namespace data"
          else
            echo ""
            echo "[WARN] No smoke_test namespace found - OpenLineage may not be fully integrated"
          fi

          echo ""
          echo "[OK] Marquez validation complete"

      - name: Test failure handling (optional)
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/airflow/dags
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
          OPENLINEAGE_URL: http://localhost:5001
          OPENLINEAGE_NAMESPACE: smoke_test_failures
        continue-on-error: true
        run: |
          echo "========================================"
          echo "Testing Failure Path (Expected to Fail)"
          echo "========================================"

          # Run with should_fail=True to test error handling
          airflow dags test smoke_test_dag 2025-01-02 \
            --conf '{"should_fail": true}' 2>&1 | tee failure_test.log || true

          if grep -q "Simulated failure" failure_test.log; then
            echo ""
            echo "[OK] Failure handling works correctly"
          else
            echo ""
            echo "[WARN] Failure test did not trigger expected error"
          fi

      - name: Validate DAG structures (infrastructure DAGs)
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/airflow/dags
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
        run: |
          echo "========================================"
          echo "Infrastructure DAG Structure Validation"
          echo "========================================"

          # Validate structure of infrastructure DAGs (can't execute without KVM)
          for dag_id in freeipa_deployment dns_management generic_vm_deployment; do
            echo ""
            echo "Checking DAG: $dag_id"

            # Show DAG details (validates structure)
            if airflow dags show $dag_id 2>/dev/null; then
              echo "[OK] $dag_id structure valid"

              # List tasks
              echo "Tasks:"
              airflow tasks list $dag_id 2>/dev/null | head -20
            else
              echo "[SKIP] $dag_id not found or has errors"
            fi
          done

          echo ""
          echo "[OK] Infrastructure DAG validation complete"

      - name: Validate OpenLineage configuration
        run: |
          echo "========================================"
          echo "OpenLineage Configuration Check"
          echo "========================================"

          # Check that OpenLineage is configured in docker-compose
          if grep -q "OPENLINEAGE_URL" airflow/docker-compose.yml; then
            echo "[OK] OPENLINEAGE_URL configured"
          else
            echo "[WARN] OPENLINEAGE_URL not found in docker-compose.yml"
          fi

          if grep -q "OPENLINEAGE_DISABLED" airflow/docker-compose.yml; then
            # Check it's set to false (enabled)
            if grep "OPENLINEAGE_DISABLED.*false" airflow/docker-compose.yml; then
              echo "[OK] OpenLineage enabled (OPENLINEAGE_DISABLED=false)"
            else
              echo "[WARN] OpenLineage may be disabled"
            fi
          fi

          # Check Marquez configuration exists
          if [[ -f "airflow/config/marquez.yml" ]]; then
            echo "[OK] Marquez config file exists"

            # Validate YAML syntax
            python3 -c "import yaml; yaml.safe_load(open('airflow/config/marquez.yml'))" && \
              echo "[OK] marquez.yml is valid YAML" || \
              echo "[ERROR] marquez.yml has invalid YAML syntax"
          else
            echo "[ERROR] airflow/config/marquez.yml not found"
            exit 1
          fi

          echo ""
          echo "[OK] OpenLineage/Marquez configuration validated"

      - name: Validate ADR-0046 SSH patterns
        run: |
          echo "========================================"
          echo "ADR-0046 SSH Pattern Validation"
          echo "========================================"

          ERRORS=0

          for dag_file in airflow/dags/*.py; do
            filename=$(basename "$dag_file")

            # Skip non-DAG files
            [[ "$filename" == __* ]] && continue
            [[ "$filename" == .* ]] && continue

            echo "Checking: $filename"

            # Check for kcli/virsh commands that are NOT wrapped in SSH
            # This is a heuristic check - looks for bare kcli/virsh in bash_command
            if grep -E 'bash_command.*kcli|bash_command.*virsh' "$dag_file" | grep -v 'ssh.*root@localhost' | grep -qv "^#"; then
              # More detailed check: find kcli/virsh that appear to be direct calls
              if grep -B10 'kcli\s' "$dag_file" | grep -q "bash_command=" && \
                 ! grep -B10 'kcli\s' "$dag_file" | grep -q "ssh.*root@localhost"; then
                echo "  [WARN] Possible bare kcli command (not SSH-wrapped)"
                # Don't fail - this is a heuristic
              fi
            fi

            # Note: Triple single quotes are allowed when bash commands contain
            # curly braces (awk, bash arrays) that conflict with f-string parsing.
            # The lint-dags.sh script handles this validation.

            echo "  [OK] Passed basic checks"
          done

          if [[ $ERRORS -gt 0 ]]; then
            echo ""
            echo "[ERROR] Found $ERRORS ADR violations"
            exit 1
          fi

          echo ""
          echo "[OK] ADR-0046 SSH pattern validation passed"

      - name: Test MCP Server (FastMCP)
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/airflow/dags
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
          OPENLINEAGE_URL: http://localhost:5001
        run: |
          echo "========================================"
          echo "MCP Server Smoke Test"
          echo "========================================"

          # Install MCP dependencies
          pip install mcp starlette uvicorn sse-starlette fastmcp 2>/dev/null || true

          # Test MCP server can import without errors
          cd airflow/scripts
          python3 -c "
          import sys
          sys.path.insert(0, '.')

          # Test import
          try:
              # Just test that the module syntax is valid
              import ast
              with open('mcp_server_fastmcp.py', 'r') as f:
                  ast.parse(f.read())
              print('[OK] MCP server syntax valid')
          except SyntaxError as e:
              print(f'[ERROR] MCP server syntax error: {e}')
              sys.exit(1)
          "

          # Test MCP server can start (quick startup test)
          echo "Testing MCP server startup..."
          timeout 10 python3 mcp_server_fastmcp.py &
          MCP_PID=$!
          sleep 3

          # Check if it's running
          if kill -0 $MCP_PID 2>/dev/null; then
            echo "[OK] MCP server started successfully"
            kill $MCP_PID 2>/dev/null || true
          else
            echo "[WARN] MCP server exited (may be expected without full environment)"
          fi

          # Test MCP server HTTP endpoint if available
          if curl -sf http://localhost:8889/sse > /dev/null 2>&1; then
            echo "[OK] MCP server SSE endpoint responding"
          else
            echo "[INFO] MCP server SSE endpoint not available (expected in CI)"
          fi

          echo ""
          echo "[OK] MCP server smoke test complete"

      - name: Print test summary
        if: always()
        run: |
          echo ""
          echo "========================================"
          echo "SMOKE TEST SUMMARY"
          echo "========================================"
          echo ""
          echo "Services tested:"
          echo "  - PostgreSQL (Airflow metadata)"
          echo "  - Airflow (DAG execution)"
          echo "  - Marquez (OpenLineage backend)"
          echo "  - MCP Server (FastMCP)"
          echo ""
          echo "Tests performed:"
          echo "  - DAG import validation"
          echo "  - smoke_test_dag execution"
          echo "  - OpenLineage event emission"
          echo "  - Marquez lineage capture"
          echo "  - Failure path handling"
          echo "  - Infrastructure DAG structure validation"
          echo "  - ADR-0045/ADR-0046 compliance"
          echo "  - MCP server syntax and startup"
          echo ""
          echo "========================================"

  # ==========================================================================
  # Full Integration Tests (requires self-hosted runner with KVM)
  # ==========================================================================
  # integration-tests:
  #   name: Integration Tests (KVM Required)
  #   runs-on: [self-hosted, kvm]
  #   needs: [validate-dags, validate-containers, smoke-test]
  #   if: github.event_name == 'push' && github.ref == 'refs/heads/main'
  #
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4
  #
  #     - name: Start Airflow stack
  #       run: |
  #         cd airflow
  #         make install
  #
  #     - name: Wait for services
  #       run: |
  #         sleep 60
  #         cd airflow
  #         make health
  #
  #     - name: Run DAG tests
  #       run: |
  #         cd airflow
  #         # Test DAG execution (requires KVM for VM operations)
  #         podman-compose exec airflow-scheduler airflow dags test freeipa_deployment 2025-01-01
  #
  #     - name: Cleanup
  #       if: always()
  #       run: |
  #         cd airflow
  #         make uninstall
