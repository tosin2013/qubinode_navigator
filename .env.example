# =============================================================================
# Qubinode Navigator Environment Configuration Template
# =============================================================================
#
# Copy this file to .env and customize the values for your deployment
# cp .env.example .env
#
# Then run: ./deploy-qubinode.sh

# =============================================================================
# REQUIRED CONFIGURATION
# =============================================================================

# Domain name for your cluster (REQUIRED)
# This will be used for DNS, certificates, and cluster access
QUBINODE_DOMAIN=example.com

# Admin username for cluster access (REQUIRED)
# This user will have administrative privileges
QUBINODE_ADMIN_USER=admin

# Cluster name (REQUIRED)
# Used to identify your Qubinode cluster
QUBINODE_CLUSTER_NAME=qubinode

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================

# Deployment mode: development, staging, production
# - development: Local development with debugging enabled
# - staging: Pre-production testing environment
# - production: Production-ready deployment (recommended)
QUBINODE_DEPLOYMENT_MODE=production

# =============================================================================
# FEATURE FLAGS (Used by inventory configurations)
# =============================================================================

# Enable AI Assistant for troubleshooting and guidance
# The AI Assistant provides intelligent help during deployment and operations
QUBINODE_ENABLE_AI_ASSISTANT=true

# Enable Ceph storage deployment (optional)
# Used in inventory group_vars - for distributed storage across multiple nodes
enable_ceph_deployment=false

# Enable Ansible Automation Platform integration
# Used in inventory group_vars - for enterprise automation workflows
ansible_automation_platform=false

# =============================================================================
# AI ASSISTANT CONFIGURATION
# =============================================================================

# Port for AI Assistant web interface
# Default: 8080, change if port conflicts exist
AI_ASSISTANT_PORT=8080

# AI Assistant container version
# Options: latest, 1.0.0, 1.0.1, etc.
AI_ASSISTANT_VERSION=latest

# -----------------------------------------------------------------------------
# AI Model Configuration (LiteLLM support)
# -----------------------------------------------------------------------------
# The AI Assistant uses LiteLLM which supports 100+ LLM providers.
# Default: granite-4.0-micro (IBM Granite 4.0 Micro, runs locally on CPU)
#
# LOCAL MODELS (llama.cpp - no API key required):
# AI_MODEL_TYPE=granite-4.0-micro    # Default: 3B params, CPU-optimized
# AI_MODEL_TYPE=granite-7b           # Larger model, needs 8GB+ RAM
# AI_MODEL_TYPE=llama3-8b            # GPU recommended (6GB+ VRAM)
# AI_MODEL_TYPE=phi3-mini            # Fast inference, GPU (4GB+ VRAM)
#
# API MODELS (requires API key):
# AI_MODEL_TYPE=openai-gpt4          # OpenAI GPT-4
# AI_MODEL_TYPE=anthropic-claude     # Anthropic Claude 3 Sonnet
# AI_MODEL_TYPE=azure-openai         # Azure OpenAI
# AI_MODEL_TYPE=ollama-local         # Local Ollama (http://localhost:11434)

# Model type selection (see presets above)
# AI_MODEL_TYPE=granite-4.0-micro

# Custom model path for local models (optional)
# AI_MODEL_PATH=/app/models/your-model.gguf

# Custom model URL for downloading local models (optional)
# AI_MODEL_URL=https://huggingface.co/your-org/your-model/resolve/main/model.gguf

# Model inference settings
# AI_MAX_TOKENS=512                  # Maximum response tokens (1-4096)
# AI_TEMPERATURE=0.7                 # Response creativity (0.0-2.0)
# AI_CONTEXT_LENGTH=8192             # Context window size
# AI_THREADS=4                       # CPU threads for local models

# GPU configuration (for local models)
# AI_USE_GPU=false                   # Enable GPU acceleration
# AI_GPU_LAYERS=0                    # Number of layers to offload to GPU

# -----------------------------------------------------------------------------
# API Keys for LiteLLM Cloud Models
# -----------------------------------------------------------------------------
# LiteLLM supports 100+ providers. Set the API key for your chosen provider.
# Docs: https://docs.litellm.ai/docs/providers

# OpenAI
# OPENAI_API_KEY=sk-your-openai-api-key

# Anthropic
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key

# Azure OpenAI
# AZURE_API_KEY=your-azure-api-key
# AZURE_API_BASE=https://your-resource.openai.azure.com
# AZURE_API_VERSION=2024-02-15-preview

# -----------------------------------------------------------------------------
# Google Gemini (RECOMMENDED - fast, cheap, capable)
# -----------------------------------------------------------------------------
# Docs: https://docs.litellm.ai/docs/providers/gemini
# Get API key: https://makersuite.google.com/app/apikey
#
# Model format: gemini/<model-name>
# Available models:
#   gemini/gemini-2.0-flash        - Fast, recommended for most tasks
#   gemini/gemini-2.0-flash-exp    - Experimental features
#   gemini/gemini-1.5-pro-latest   - More capable, higher latency
#   gemini/gemini-1.5-flash        - Balance of speed and capability
#   gemini/gemini-pro              - Basic model
#
# GEMINI_API_KEY=your-gemini-api-key

# Google Vertex AI (alternative to Gemini API - enterprise)
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
# VERTEXAI_PROJECT=your-gcp-project
# VERTEXAI_LOCATION=us-central1

# -----------------------------------------------------------------------------
# OpenRouter (access 100+ models via single API)
# -----------------------------------------------------------------------------
# Docs: https://docs.litellm.ai/docs/providers/openrouter
# Get API key: https://openrouter.ai/keys
#
# Model format: openrouter/<provider>/<model-name>
# Example models:
#   openrouter/google/gemini-2.0-flash-001      - Gemini via OpenRouter
#   openrouter/anthropic/claude-3.5-sonnet      - Claude via OpenRouter
#   openrouter/openai/gpt-4o                    - GPT-4o via OpenRouter
#   openrouter/meta-llama/llama-3.3-70b-instruct - Llama 3.3 70B
#   openrouter/deepseek/deepseek-r1             - DeepSeek R1
#
# OPENROUTER_API_KEY=sk-or-your-openrouter-api-key
# OR_SITE_URL=https://your-site.com            # Optional: your site URL
# OR_APP_NAME=qubinode-navigator               # Optional: your app name

# -----------------------------------------------------------------------------
# Ollama (local models - free, no API key required)
# -----------------------------------------------------------------------------
# Docs: https://docs.litellm.ai/docs/providers/ollama
# Setup: https://ollama.com/download
#
# Model format: ollama/<model-name> or ollama_chat/<model-name>
# Use ollama_chat/ for chat completions (recommended)
# Available models (run `ollama list` to see installed):
#   ollama/llama3.2:latest           - Meta Llama 3.2
#   ollama/granite3.3:8b             - IBM Granite 3.3 8B
#   ollama/codellama:13b             - Code-specialized Llama
#   ollama/mistral:7b                - Mistral 7B
#   ollama/mixtral:8x7b              - Mixtral 8x7B
#   ollama/llava                     - Vision model
#
# OLLAMA_BASE_URL=http://localhost:11434

# -----------------------------------------------------------------------------
# vLLM (high-performance local inference)
# -----------------------------------------------------------------------------
# Docs: https://docs.litellm.ai/docs/providers/vllm
# vLLM server: https://docs.vllm.ai/en/latest/
#
# Model format: hosted_vllm/<model-name>
# Example models:
#   hosted_vllm/meta-llama/Llama-3.3-70B-Instruct
#   hosted_vllm/ibm-granite/granite-3.1-8b-instruct
#   hosted_vllm/mistralai/Mistral-7B-Instruct-v0.3
#
# HOSTED_VLLM_API_BASE=http://localhost:8000/v1
# HOSTED_VLLM_API_KEY=token-optional-for-auth

# -----------------------------------------------------------------------------
# Anthropic Claude (direct API)
# -----------------------------------------------------------------------------
# Docs: https://docs.litellm.ai/docs/providers/anthropic
# Get API key: https://console.anthropic.com/
#
# Model format: anthropic/<model-name>
# Available models:
#   anthropic/claude-sonnet-4-5-20250929   - Claude 4.5 Sonnet (latest)
#   anthropic/claude-opus-4-1-20250805     - Claude 4.1 Opus
#   anthropic/claude-3-5-sonnet-20241022   - Claude 3.5 Sonnet
#   anthropic/claude-3-haiku-20240307      - Claude 3 Haiku (fast, cheap)
#   anthropic/claude-3-opus-20240229       - Claude 3 Opus
#
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key

# Together AI
# TOGETHERAI_API_KEY=your-together-api-key

# Groq (fast inference)
# GROQ_API_KEY=gsk_your-groq-api-key

# -----------------------------------------------------------------------------
# PydanticAI Multi-Agent Model Configuration (ADR-0049/ADR-0063)
# -----------------------------------------------------------------------------
# The AI Assistant uses a multi-agent architecture with separate models
# for orchestration vs coding tasks. This allows using a smarter model
# for planning and a faster/cheaper model for execution.
#
# IMPORTANT: PydanticAI uses "provider:model" format (NOT LiteLLM format!)
# PydanticAI docs: https://ai.pydantic.dev/models/
#
# PydanticAI Model Format Quick Reference:
#   google-gla:gemini-2.0-flash          - Google Gemini (fast, cheap) - RECOMMENDED
#   google-gla:gemini-2.5-flash          - Google Gemini 2.5 Flash
#   google-gla:gemini-1.5-pro            - Google Gemini 1.5 Pro
#   openai:gpt-4o                        - OpenAI GPT-4o
#   openai:gpt-4-turbo                   - OpenAI GPT-4 Turbo
#   anthropic:claude-3-5-sonnet-latest   - Anthropic Claude 3.5 Sonnet
#   anthropic:claude-3-opus-latest       - Anthropic Claude 3 Opus
#   anthropic:claude-3-haiku-20240307    - Anthropic Claude 3 Haiku (fast)
#   ollama:granite3.3:8b                 - Local Ollama Granite 3.3
#   ollama:llama3.2                      - Local Ollama Llama 3.2
#   groq:llama-3.3-70b-versatile         - Groq Llama 3.3 70B
#
#   OpenRouter (access 100+ models via single API key):
#   openrouter:anthropic/claude-3.5-sonnet    - Claude 3.5 Sonnet via OpenRouter
#   openrouter:anthropic/claude-3-haiku       - Claude 3 Haiku via OpenRouter
#   openrouter:google/gemini-2.0-flash-exp    - Gemini 2.0 Flash via OpenRouter
#   openrouter:openai/gpt-4o                  - GPT-4o via OpenRouter
#   openrouter:meta-llama/llama-3.1-70b-instruct - Llama 3.1 70B via OpenRouter
#   openrouter:deepseek/deepseek-chat         - DeepSeek Chat via OpenRouter
#
# IMPORTANT: PydanticAI uses "provider:model" format with COLON separator
#   - Correct:   openrouter:anthropic/claude-3.5-sonnet
#   - WRONG:     openrouter/anthropic/claude-3.5-sonnet (LiteLLM format)
#
# Note: For Gemini, use google-gla: prefix (Google AI Studio API)
#       For Google Cloud Vertex AI, use google-vertex: prefix

# ORCHESTRATOR MODEL (Manager Agent)
# Used for: Planning, task decomposition, escalation decisions
# Recommended: Use a capable model for better planning quality
MANAGER_MODEL=google-gla:gemini-3-pro-preview
# MANAGER_MODEL=google-gla:gemini-2.5-flash
# MANAGER_MODEL=anthropic:claude-3-5-sonnet-latest
# MANAGER_MODEL=openai:gpt-4o
# MANAGER_MODEL=ollama:granite3.3:8b
# MANAGER_MODEL=openrouter:anthropic/claude-3.5-sonnet

# CODING/WORKER MODEL (Developer Agent)
# Used for: Code generation, task execution, configuration changes
# Recommended: Can use faster/cheaper model since tasks are well-defined
DEVELOPER_MODEL=google-gla:gemini-3-pro-preview
# DEVELOPER_MODEL=google-gla:gemini-2.5-flash
# DEVELOPER_MODEL=groq:llama-3.3-70b-versatile
# DEVELOPER_MODEL=ollama:granite3.3:8b
# DEVELOPER_MODEL=openrouter:google/gemini-2.0-flash-exp

# DEPLOYMENT MODEL (Infrastructure Agent)
# Used for: VM provisioning, cluster deployments, service configurations
PYDANTICAI_MODEL=google-gla:gemini-3-pro-preview
# PYDANTICAI_MODEL=anthropic:claude-3-5-sonnet-latest
# PYDANTICAI_MODEL=openai:gpt-4o
# PYDANTICAI_MODEL=openrouter:anthropic/claude-3-haiku

# CE-MCP Server URL for agent tool access (ADR-0064)
# QUBINODE_MCP_URL=http://localhost:8890/mcp

# Logfire observability (optional - for agent tracing)
# LOGFIRE_TOKEN=your-logfire-token

# =============================================================================
# ADVANCED CONFIGURATION (Optional)
# =============================================================================

# Custom container registry (optional)
# Leave empty to use default quay.io registry
# QUBINODE_REGISTRY=your-registry.com

# Custom network CIDR (optional)
# Default network configuration will be used if not specified
# QUBINODE_NETWORK_CIDR=10.0.0.0/16

# Custom DNS servers (optional)
# Comma-separated list of DNS servers
# QUBINODE_DNS_SERVERS=8.8.8.8,8.8.4.4

# HashiCorp Vault Configuration (optional)
# Uncomment and configure if using HashiCorp Vault for secrets management
# VAULT_ADDRESS=https://vault.company.com
# VAULT_TOKEN=your_vault_token
# SECRET_PATH=secret/qubinode

# HashiCorp Cloud Platform (HCP) Vault Configuration (optional)
# Uncomment and configure if using HCP Vault Secrets
# HCP_CLIENT_ID=your_client_id
# HCP_CLIENT_SECRET=your_client_secret
# HCP_ORG_ID=your_org_id
# HCP_PROJECT_ID=your_project_id
# APP_NAME=your_app_name

# Proxy configuration (optional)
# Uncomment and configure if behind a corporate proxy
# HTTP_PROXY=http://proxy.company.com:8080
# HTTPS_PROXY=http://proxy.company.com:8080
# NO_PROXY=localhost,127.0.0.1,.company.com

# =============================================================================
# DEPLOYMENT-SPECIFIC CONFIGURATION
# =============================================================================

# Deployment Environment Variables (based on existing deployment patterns)
# These match the patterns used in demo-hetzner-com.markdown and demo-redhat-com.markdown

# SSH Configuration
# SSH_PASSWORD=your_ssh_password  # Set this for automated SSH key copying

# Network Configuration
# FORWARDER=1.1.1.1  # DNS forwarder (Hetzner: 1.1.1.1, Equinix: auto-detect)
# INTERFACE=bond0     # Network interface (common: bond0, eth0, ens3)
# ACTIVE_BRIDGE=false # Whether to use active bridge configuration

# CI/CD Configuration
# CICD_ENVIORNMENT=gitlab  # CI/CD environment (gitlab, onedev, github)
# ENV_USERNAME=lab-user    # Environment username

# KVM Configuration
KVM_VERSION=0.10.4          # KVM version to use

# Route53 Configuration (if using AWS Route53)
# USE_ROUTE53=true         # Enable Route53 integration
# ZONE_NAME=your-domain.com # Route53 zone name
# EMAIL=user@example.com   # Email for Let's Encrypt certificates
# GUID=your-guid          # Unique identifier for the deployment

# =============================================================================
# CLOUD PROVIDER CONFIGURATION (Optional)
# =============================================================================

# Uncomment the section for your cloud provider if deploying to cloud

# AWS Configuration
# AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key

# Azure Configuration
# AZURE_SUBSCRIPTION_ID=your_subscription_id
# AZURE_TENANT_ID=your_tenant_id
# AZURE_CLIENT_ID=your_client_id
# AZURE_CLIENT_SECRET=your_client_secret

# GCP Configuration
# GCP_PROJECT_ID=your_project_id
# GCP_SERVICE_ACCOUNT_KEY_PATH=/path/to/service-account.json

# Equinix Metal Configuration
# EQUINIX_API_TOKEN=your_api_token
# EQUINIX_PROJECT_ID=your_project_id

# Hetzner Cloud Configuration
# HETZNER_TOKEN=your_hetzner_token

# =============================================================================
# DEPLOYMENT-SPECIFIC VARIABLES (Used by deployment target detection)
# =============================================================================

# SSH Configuration
# SSH_PASSWORD=your_ssh_password  # Set this for automated SSH key copying

# Network Configuration (auto-detected based on deployment target)
# FORWARDER=1.1.1.1  # DNS forwarder (Hetzner: 1.1.1.1, Equinix: auto-detect)
# INTERFACE=bond0     # Network interface (common: bond0, eth0, ens3)
# ACTIVE_BRIDGE=false # Whether to use active bridge configuration

# CI/CD Configuration
# CICD_ENVIORNMENT=gitlab  # CI/CD environment (gitlab, onedev, github)
# ENV_USERNAME=lab-user    # Environment username

# KVM Configuration
# KVM_VERSION=0.8.0        # KVM version to use

# Route53 Configuration (if using AWS Route53)
# USE_ROUTE53=true         # Enable Route53 integration
# ZONE_NAME=your-domain.com # Route53 zone name
# EMAIL=user@example.com   # Email for Let's Encrypt certificates
# GUID=your-guid          # Unique identifier for the deployment

# =============================================================================
# HASHICORP VAULT INTEGRATION (Optional)
# =============================================================================

# HashiCorp Cloud Platform (HCP) Vault Configuration (optional)
# Uncomment and configure if using HCP Vault Secrets
# HCP_CLIENT_ID=your_client_id
# HCP_CLIENT_SECRET=your_client_secret
# HCP_ORG_ID=your_org_id
# HCP_PROJECT_ID=your_project_id
# APP_NAME=your_app_name

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# Example 1: Hetzner Cloud Deployment
# QUBINODE_DOMAIN=qubinodelab.io
# QUBINODE_ADMIN_USER=lab-user
# QUBINODE_CLUSTER_NAME=hetzner-hypervisor
# QUBINODE_DEPLOYMENT_MODE=production
# INVENTORY=hetzner
# SSH_USER=lab-user
# CICD_PIPELINE=true
# USE_HASHICORP_VAULT=false
# HETZNER_TOKEN=your_hetzner_token

# Example 2: Red Hat Demo System (Equinix Metal)
# QUBINODE_DOMAIN=sandbox000.opentlc.com
# QUBINODE_ADMIN_USER=lab-user
# QUBINODE_CLUSTER_NAME=rhel9-equinix-hypervisor
# QUBINODE_DEPLOYMENT_MODE=production
# INVENTORY=rhel9-equinix
# SSH_USER=lab-user
# CICD_PIPELINE=true
# USE_HASHICORP_VAULT=false
# EQUINIX_API_TOKEN=your_api_token
# EQUINIX_PROJECT_ID=your_project_id

# Example 3: Local Development Setup
# QUBINODE_DOMAIN=dev.local
# QUBINODE_ADMIN_USER=developer
# QUBINODE_CLUSTER_NAME=dev-hypervisor
# QUBINODE_DEPLOYMENT_MODE=development
# INVENTORY=localhost
# SSH_USER=developer
# CICD_PIPELINE=false
# USE_HASHICORP_VAULT=false

# Example 4: Production Setup with HashiCorp Vault
# QUBINODE_DOMAIN=prod.company.com
# QUBINODE_ADMIN_USER=admin
# QUBINODE_CLUSTER_NAME=prod-hypervisor
# QUBINODE_DEPLOYMENT_MODE=production
# SSH_USER=admin
# CICD_PIPELINE=true
# USE_HASHICORP_VAULT=true
# USE_HASHICORP_CLOUD=true
# HCP_CLIENT_ID=your_client_id
# HCP_CLIENT_SECRET=your_client_secret
# HCP_ORG_ID=your_org_id
# HCP_PROJECT_ID=your_project_id
# APP_NAME=appname
# enable_ceph_deployment=true
# ansible_automation_platform=true

# =============================================================================
# DEPLOYMENT TARGET CONFIGURATIONS
# =============================================================================

# Based on existing deployment documentation patterns:

# HETZNER CLOUD DEPLOYMENT
# Use these settings for Hetzner Cloud deployments (demo-hetzner-com.markdown)
# INVENTORY=hetzner
# QUBINODE_DOMAIN=qubinodelab.io
# FORWARDER=1.1.1.1
# INTERFACE=bond0
# Uses: rocky-linux-hetzner.sh script

# RED HAT DEMO SYSTEM (EQUINIX METAL)
# Use these settings for Red Hat Demo System deployments (demo-redhat-com.markdown)
# INVENTORY=rhel9-equinix
# QUBINODE_DOMAIN=sandbox000.opentlc.com
# FORWARDER=$(awk '/^nameserver/ {print $2}' /etc/resolv.conf | head -1)
# INTERFACE=bond0
# Uses: rhel9-linux-hypervisor.sh script

# LOCAL DEVELOPMENT
# Use these settings for local development
# INVENTORY=localhost
# QUBINODE_DOMAIN=dev.local
# Uses: setup.sh -> rhel9-linux-hypervisor.sh workflow

# =============================================================================
# VALIDATION NOTES
# =============================================================================
#
# Required Variables:
# - QUBINODE_DOMAIN: Must be a valid domain name
# - QUBINODE_ADMIN_USER: Must be a valid username (no special characters)
# - QUBINODE_CLUSTER_NAME: Must be a valid cluster name (alphanumeric, hyphens)
# - INVENTORY: Must match one of the available inventories (localhost, hetzner, rhel9-equinix, etc.)
#
# Resource Requirements:
# - Minimum 8GB RAM recommended
# - Minimum 50GB disk space
# - Internet connectivity required
# - Hardware virtualization support (VT-x/AMD-V)
#
# Network Requirements:
# - Outbound internet access for container pulls
# - DNS resolution working
# - No conflicting services on configured ports
#
# Deployment-Specific Requirements:
# - SSH_PASSWORD: Required for automated SSH key setup
# - Cloud credentials: Required for cloud deployments (Hetzner, AWS, etc.)
# - RHSM credentials: Required for RHEL subscriptions (via /tmp/config.yml)
#
# =============================================================================

# =============================================================================
# STORAGE CONFIGURATION - LVM and Libvirt
# =============================================================================

# Enable LVM volume group creation
# Set to false to skip LVM creation if you already have storage configured
# or if you want to manage storage manually
# CREATE_LVM=true

# Enable libvirt storage pool configuration
# Set to false to skip libvirt storage pool setup
# CREATE_LIBVIRT_STORAGE=true

# Storage disk for KVM (LVM and libvirt)
# Leave empty for auto-detection, or specify device like /dev/nvme0n1
# KVM_HOST_LIBVIRT_EXTRA_DISK=/dev/nvme0n1
