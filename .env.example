# =============================================================================
# Qubinode Navigator Environment Configuration Template
# =============================================================================
#
# Copy this file to .env and customize the values for your deployment
# cp .env.example .env
#
# Then run: ./deploy-qubinode.sh

# =============================================================================
# REQUIRED CONFIGURATION
# =============================================================================

# Domain name for your cluster (REQUIRED)
# This will be used for DNS, certificates, and cluster access
QUBINODE_DOMAIN=example.com

# Admin username for cluster access (REQUIRED)
# This user will have administrative privileges
QUBINODE_ADMIN_USER=admin

# Cluster name (REQUIRED)
# Used to identify your Qubinode cluster
QUBINODE_CLUSTER_NAME=qubinode

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================

# Deployment mode: development, staging, production
# - development: Local development with debugging enabled
# - staging: Pre-production testing environment
# - production: Production-ready deployment (recommended)
QUBINODE_DEPLOYMENT_MODE=production

# =============================================================================
# FEATURE FLAGS (Used by inventory configurations)
# =============================================================================

# Enable AI Assistant for troubleshooting and guidance
# The AI Assistant provides intelligent help during deployment and operations
QUBINODE_ENABLE_AI_ASSISTANT=true

# Enable Ceph storage deployment (optional)
# Used in inventory group_vars - for distributed storage across multiple nodes
enable_ceph_deployment=false

# Enable Ansible Automation Platform integration
# Used in inventory group_vars - for enterprise automation workflows
ansible_automation_platform=false

# =============================================================================
# AI ASSISTANT CONFIGURATION
# =============================================================================

# Port for AI Assistant web interface
# Default: 8080, change if port conflicts exist
AI_ASSISTANT_PORT=8080

# AI Assistant container version
# Options: latest, 1.0.0, 1.0.1, etc.
AI_ASSISTANT_VERSION=latest

# -----------------------------------------------------------------------------
# AI Model Configuration (LiteLLM support)
# -----------------------------------------------------------------------------
# The AI Assistant uses LiteLLM which supports 100+ LLM providers.
# Default: granite-4.0-micro (IBM Granite 4.0 Micro, runs locally on CPU)
#
# LOCAL MODELS (llama.cpp - no API key required):
# AI_MODEL_TYPE=granite-4.0-micro    # Default: 3B params, CPU-optimized
# AI_MODEL_TYPE=granite-7b           # Larger model, needs 8GB+ RAM
# AI_MODEL_TYPE=llama3-8b            # GPU recommended (6GB+ VRAM)
# AI_MODEL_TYPE=phi3-mini            # Fast inference, GPU (4GB+ VRAM)
#
# API MODELS (requires API key):
# AI_MODEL_TYPE=openai-gpt4          # OpenAI GPT-4
# AI_MODEL_TYPE=anthropic-claude     # Anthropic Claude 3 Sonnet
# AI_MODEL_TYPE=azure-openai         # Azure OpenAI
# AI_MODEL_TYPE=ollama-local         # Local Ollama (http://localhost:11434)

# Model type selection (see presets above)
# AI_MODEL_TYPE=granite-4.0-micro

# Custom model path for local models (optional)
# AI_MODEL_PATH=/app/models/your-model.gguf

# Custom model URL for downloading local models (optional)
# AI_MODEL_URL=https://huggingface.co/your-org/your-model/resolve/main/model.gguf

# Model inference settings
# AI_MAX_TOKENS=512                  # Maximum response tokens (1-4096)
# AI_TEMPERATURE=0.7                 # Response creativity (0.0-2.0)
# AI_CONTEXT_LENGTH=8192             # Context window size
# AI_THREADS=4                       # CPU threads for local models

# GPU configuration (for local models)
# AI_USE_GPU=false                   # Enable GPU acceleration
# AI_GPU_LAYERS=0                    # Number of layers to offload to GPU

# -----------------------------------------------------------------------------
# API Keys for LiteLLM Cloud Models
# -----------------------------------------------------------------------------
# Uncomment and set the key for your chosen cloud provider

# OpenAI (for AI_MODEL_TYPE=openai-gpt4)
# OPENAI_API_KEY=sk-your-openai-api-key

# Anthropic (for AI_MODEL_TYPE=anthropic-claude)
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key

# Azure OpenAI (for AI_MODEL_TYPE=azure-openai)
# AZURE_API_KEY=your-azure-api-key
# AZURE_API_BASE=https://your-resource.openai.azure.com
# AZURE_API_VERSION=2024-02-15-preview

# Google Vertex AI
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
# VERTEXAI_PROJECT=your-gcp-project
# VERTEXAI_LOCATION=us-central1

# Ollama (local, for AI_MODEL_TYPE=ollama-local)
# OLLAMA_API_BASE=http://localhost:11434

# OpenRouter (access to multiple models via single API)
# OPENROUTER_API_KEY=sk-or-your-openrouter-api-key

# Together AI
# TOGETHERAI_API_KEY=your-together-api-key

# Groq (fast inference)
# GROQ_API_KEY=gsk_your-groq-api-key

# -----------------------------------------------------------------------------
# PydanticAI Multi-Agent Model Configuration (ADR-0049/ADR-0063)
# -----------------------------------------------------------------------------
# The AI Assistant uses a multi-agent architecture with separate models
# for orchestration vs coding tasks. This allows using a smarter model
# for planning and a faster/cheaper model for execution.
#
# Model format: "provider:model_name" (e.g., "ollama:granite3.3:8b", "anthropic:claude-sonnet-4-5")
# Supported providers: ollama, anthropic, openai, google, groq, together, openrouter

# ORCHESTRATOR MODEL (Manager Agent)
# Used for: Planning, task decomposition, escalation decisions
# Default: ollama:granite3.3:8b (local, free)
# Recommended: Use a smarter model for better planning quality
# MANAGER_MODEL=ollama:granite3.3:8b
# MANAGER_MODEL=anthropic:claude-sonnet-4-5
# MANAGER_MODEL=openai:gpt-4o

# CODING/WORKER MODEL (Developer Agent)
# Used for: Code generation, task execution, configuration changes
# Default: ollama:granite3.3:8b (local, free)
# Recommended: Can use faster/cheaper model since tasks are well-defined
# DEVELOPER_MODEL=ollama:granite3.3:8b
# DEVELOPER_MODEL=anthropic:claude-sonnet-4-5
# DEVELOPER_MODEL=groq:llama-3.3-70b-versatile

# DEPLOYMENT MODEL (Infrastructure Agent)
# Used for: VM provisioning, cluster deployments, service configurations
# Default: anthropic:claude-sonnet-4-5 (requires ANTHROPIC_API_KEY)
# PYDANTICAI_MODEL=anthropic:claude-sonnet-4-5
# PYDANTICAI_MODEL=openai:gpt-4o

# MCP Server URL for agent tool access
# QUBINODE_MCP_URL=http://localhost:8889/mcp

# Logfire observability (optional - for agent tracing)
# LOGFIRE_TOKEN=your-logfire-token

# =============================================================================
# ADVANCED CONFIGURATION (Optional)
# =============================================================================

# Custom container registry (optional)
# Leave empty to use default quay.io registry
# QUBINODE_REGISTRY=your-registry.com

# Custom network CIDR (optional)
# Default network configuration will be used if not specified
# QUBINODE_NETWORK_CIDR=10.0.0.0/16

# Custom DNS servers (optional)
# Comma-separated list of DNS servers
# QUBINODE_DNS_SERVERS=8.8.8.8,8.8.4.4

# HashiCorp Vault Configuration (optional)
# Uncomment and configure if using HashiCorp Vault for secrets management
# VAULT_ADDRESS=https://vault.company.com
# VAULT_TOKEN=your_vault_token
# SECRET_PATH=secret/qubinode

# HashiCorp Cloud Platform (HCP) Vault Configuration (optional)
# Uncomment and configure if using HCP Vault Secrets
# HCP_CLIENT_ID=your_client_id
# HCP_CLIENT_SECRET=your_client_secret
# HCP_ORG_ID=your_org_id
# HCP_PROJECT_ID=your_project_id
# APP_NAME=your_app_name

# Proxy configuration (optional)
# Uncomment and configure if behind a corporate proxy
# HTTP_PROXY=http://proxy.company.com:8080
# HTTPS_PROXY=http://proxy.company.com:8080
# NO_PROXY=localhost,127.0.0.1,.company.com

# =============================================================================
# DEPLOYMENT-SPECIFIC CONFIGURATION
# =============================================================================

# Deployment Environment Variables (based on existing deployment patterns)
# These match the patterns used in demo-hetzner-com.markdown and demo-redhat-com.markdown

# SSH Configuration
# SSH_PASSWORD=your_ssh_password  # Set this for automated SSH key copying

# Network Configuration
# FORWARDER=1.1.1.1  # DNS forwarder (Hetzner: 1.1.1.1, Equinix: auto-detect)
# INTERFACE=bond0     # Network interface (common: bond0, eth0, ens3)
# ACTIVE_BRIDGE=false # Whether to use active bridge configuration

# CI/CD Configuration
# CICD_ENVIORNMENT=gitlab  # CI/CD environment (gitlab, onedev, github)
# ENV_USERNAME=lab-user    # Environment username

# KVM Configuration
KVM_VERSION=0.10.4          # KVM version to use

# Route53 Configuration (if using AWS Route53)
# USE_ROUTE53=true         # Enable Route53 integration
# ZONE_NAME=your-domain.com # Route53 zone name
# EMAIL=user@example.com   # Email for Let's Encrypt certificates
# GUID=your-guid          # Unique identifier for the deployment

# =============================================================================
# CLOUD PROVIDER CONFIGURATION (Optional)
# =============================================================================

# Uncomment the section for your cloud provider if deploying to cloud

# AWS Configuration
# AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key

# Azure Configuration
# AZURE_SUBSCRIPTION_ID=your_subscription_id
# AZURE_TENANT_ID=your_tenant_id
# AZURE_CLIENT_ID=your_client_id
# AZURE_CLIENT_SECRET=your_client_secret

# GCP Configuration
# GCP_PROJECT_ID=your_project_id
# GCP_SERVICE_ACCOUNT_KEY_PATH=/path/to/service-account.json

# Equinix Metal Configuration
# EQUINIX_API_TOKEN=your_api_token
# EQUINIX_PROJECT_ID=your_project_id

# Hetzner Cloud Configuration
# HETZNER_TOKEN=your_hetzner_token

# =============================================================================
# DEPLOYMENT-SPECIFIC VARIABLES (Used by deployment target detection)
# =============================================================================

# SSH Configuration
# SSH_PASSWORD=your_ssh_password  # Set this for automated SSH key copying

# Network Configuration (auto-detected based on deployment target)
# FORWARDER=1.1.1.1  # DNS forwarder (Hetzner: 1.1.1.1, Equinix: auto-detect)
# INTERFACE=bond0     # Network interface (common: bond0, eth0, ens3)
# ACTIVE_BRIDGE=false # Whether to use active bridge configuration

# CI/CD Configuration
# CICD_ENVIORNMENT=gitlab  # CI/CD environment (gitlab, onedev, github)
# ENV_USERNAME=lab-user    # Environment username

# KVM Configuration
# KVM_VERSION=0.8.0        # KVM version to use

# Route53 Configuration (if using AWS Route53)
# USE_ROUTE53=true         # Enable Route53 integration
# ZONE_NAME=your-domain.com # Route53 zone name
# EMAIL=user@example.com   # Email for Let's Encrypt certificates
# GUID=your-guid          # Unique identifier for the deployment

# =============================================================================
# HASHICORP VAULT INTEGRATION (Optional)
# =============================================================================

# HashiCorp Cloud Platform (HCP) Vault Configuration (optional)
# Uncomment and configure if using HCP Vault Secrets
# HCP_CLIENT_ID=your_client_id
# HCP_CLIENT_SECRET=your_client_secret
# HCP_ORG_ID=your_org_id
# HCP_PROJECT_ID=your_project_id
# APP_NAME=your_app_name

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# Example 1: Hetzner Cloud Deployment
# QUBINODE_DOMAIN=qubinodelab.io
# QUBINODE_ADMIN_USER=lab-user
# QUBINODE_CLUSTER_NAME=hetzner-hypervisor
# QUBINODE_DEPLOYMENT_MODE=production
# INVENTORY=hetzner
# SSH_USER=lab-user
# CICD_PIPELINE=true
# USE_HASHICORP_VAULT=false
# HETZNER_TOKEN=your_hetzner_token

# Example 2: Red Hat Demo System (Equinix Metal)
# QUBINODE_DOMAIN=sandbox000.opentlc.com
# QUBINODE_ADMIN_USER=lab-user
# QUBINODE_CLUSTER_NAME=rhel9-equinix-hypervisor
# QUBINODE_DEPLOYMENT_MODE=production
# INVENTORY=rhel9-equinix
# SSH_USER=lab-user
# CICD_PIPELINE=true
# USE_HASHICORP_VAULT=false
# EQUINIX_API_TOKEN=your_api_token
# EQUINIX_PROJECT_ID=your_project_id

# Example 3: Local Development Setup
# QUBINODE_DOMAIN=dev.local
# QUBINODE_ADMIN_USER=developer
# QUBINODE_CLUSTER_NAME=dev-hypervisor
# QUBINODE_DEPLOYMENT_MODE=development
# INVENTORY=localhost
# SSH_USER=developer
# CICD_PIPELINE=false
# USE_HASHICORP_VAULT=false

# Example 4: Production Setup with HashiCorp Vault
# QUBINODE_DOMAIN=prod.company.com
# QUBINODE_ADMIN_USER=admin
# QUBINODE_CLUSTER_NAME=prod-hypervisor
# QUBINODE_DEPLOYMENT_MODE=production
# SSH_USER=admin
# CICD_PIPELINE=true
# USE_HASHICORP_VAULT=true
# USE_HASHICORP_CLOUD=true
# HCP_CLIENT_ID=your_client_id
# HCP_CLIENT_SECRET=your_client_secret
# HCP_ORG_ID=your_org_id
# HCP_PROJECT_ID=your_project_id
# APP_NAME=appname
# enable_ceph_deployment=true
# ansible_automation_platform=true

# =============================================================================
# DEPLOYMENT TARGET CONFIGURATIONS
# =============================================================================

# Based on existing deployment documentation patterns:

# HETZNER CLOUD DEPLOYMENT
# Use these settings for Hetzner Cloud deployments (demo-hetzner-com.markdown)
# INVENTORY=hetzner
# QUBINODE_DOMAIN=qubinodelab.io
# FORWARDER=1.1.1.1
# INTERFACE=bond0
# Uses: rocky-linux-hetzner.sh script

# RED HAT DEMO SYSTEM (EQUINIX METAL)
# Use these settings for Red Hat Demo System deployments (demo-redhat-com.markdown)
# INVENTORY=rhel9-equinix
# QUBINODE_DOMAIN=sandbox000.opentlc.com
# FORWARDER=$(awk '/^nameserver/ {print $2}' /etc/resolv.conf | head -1)
# INTERFACE=bond0
# Uses: rhel9-linux-hypervisor.sh script

# LOCAL DEVELOPMENT
# Use these settings for local development
# INVENTORY=localhost
# QUBINODE_DOMAIN=dev.local
# Uses: setup.sh -> rhel9-linux-hypervisor.sh workflow

# =============================================================================
# VALIDATION NOTES
# =============================================================================
#
# Required Variables:
# - QUBINODE_DOMAIN: Must be a valid domain name
# - QUBINODE_ADMIN_USER: Must be a valid username (no special characters)
# - QUBINODE_CLUSTER_NAME: Must be a valid cluster name (alphanumeric, hyphens)
# - INVENTORY: Must match one of the available inventories (localhost, hetzner, rhel9-equinix, etc.)
#
# Resource Requirements:
# - Minimum 8GB RAM recommended
# - Minimum 50GB disk space
# - Internet connectivity required
# - Hardware virtualization support (VT-x/AMD-V)
#
# Network Requirements:
# - Outbound internet access for container pulls
# - DNS resolution working
# - No conflicting services on configured ports
#
# Deployment-Specific Requirements:
# - SSH_PASSWORD: Required for automated SSH key setup
# - Cloud credentials: Required for cloud deployments (Hetzner, AWS, etc.)
# - RHSM credentials: Required for RHEL subscriptions (via /tmp/config.yml)
#
# =============================================================================

# =============================================================================
# STORAGE CONFIGURATION - LVM and Libvirt
# =============================================================================

# Enable LVM volume group creation
# Set to false to skip LVM creation if you already have storage configured
# or if you want to manage storage manually
# CREATE_LVM=true

# Enable libvirt storage pool configuration
# Set to false to skip libvirt storage pool setup
# CREATE_LIBVIRT_STORAGE=true

# Storage disk for KVM (LVM and libvirt)
# Leave empty for auto-detection, or specify device like /dev/nvme0n1
# KVM_HOST_LIBVIRT_EXTRA_DISK=/dev/nvme0n1
