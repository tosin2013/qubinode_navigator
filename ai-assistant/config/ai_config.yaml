# Qubinode AI Assistant Configuration
# Based on ADR-0027: CPU-Based AI Deployment Assistant Architecture

ai_service:
  # Model configuration - can be overridden with environment variables
  model_type: "${AI_MODEL_TYPE:-granite-4.0-micro}"  # granite-4.0-micro, granite-7b, llama3-8b, custom
  model_path: "${AI_MODEL_PATH:-/app/models/granite-4.0-micro.gguf}"
  model_url: "${AI_MODEL_URL:-https://huggingface.co/bartowski/granite-3.0-2b-instruct-GGUF/resolve/main/granite-3.0-2b-instruct-Q4_K_M.gguf}"
  
  # Hardware configuration
  use_gpu: "${AI_USE_GPU:-false}"
  gpu_layers: "${AI_GPU_LAYERS:-0}"  # Number of layers to offload to GPU
  threads: "${AI_THREADS:-4}"  # CPU threads to use
  
  # Server configuration
  llama_server_port: 8081
  context_length: "${AI_CONTEXT_LENGTH:-4096}"
  temperature: "${AI_TEMPERATURE:-0.7}"
  max_tokens: "${AI_MAX_TOKENS:-512}"
  
  # Model presets for different hardware configurations
  model_presets:
    # CPU-optimized models
    granite-4.0-micro:
      model_url: "https://huggingface.co/bartowski/granite-3.0-2b-instruct-GGUF/resolve/main/granite-3.0-2b-instruct-Q4_K_M.gguf"
      model_path: "/app/models/granite-4.0-micro.gguf"
      context_length: 4096
      recommended_for: "CPU-only, low memory (2GB+)"
    
    granite-7b:
      model_url: "https://huggingface.co/instructlab/granite-7b-lab-GGUF/resolve/main/granite-7b-lab.Q4_K_M.gguf"
      model_path: "/app/models/granite-7b.gguf"
      context_length: 8192
      recommended_for: "CPU with 8GB+ RAM or GPU"
    
    # GPU-optimized models
    llama3-8b:
      model_url: "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
      model_path: "/app/models/llama3-8b.gguf"
      context_length: 8192
      gpu_layers: 32
      recommended_for: "GPU with 6GB+ VRAM"
    
    phi3-mini:
      model_url: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
      model_path: "/app/models/phi3-mini.gguf"
      context_length: 4096
      gpu_layers: 32
      recommended_for: "GPU with 4GB+ VRAM, fast inference"
    
    # LiteLLM-supported models (API-based)
    openai-gpt4:
      provider: "litellm"
      model_name: "gpt-4"
      api_endpoint: "https://api.openai.com/v1"
      recommended_for: "Cloud deployment with OpenAI API access"
      
    anthropic-claude:
      provider: "litellm"
      model_name: "claude-3-sonnet-20240229"
      api_endpoint: "https://api.anthropic.com"
      recommended_for: "Cloud deployment with Anthropic API access"
      
    azure-openai:
      provider: "litellm"
      model_name: "azure/gpt-4"
      api_endpoint: "${AZURE_API_BASE}"
      recommended_for: "Azure OpenAI deployment"
      
    ollama-local:
      provider: "litellm"
      model_name: "ollama/llama3"
      api_endpoint: "http://localhost:11434"
      recommended_for: "Local Ollama deployment with GPU"

server:
  host: "0.0.0.0"
  port: 8080
  log_level: "INFO"
  timeout: 30

features:
  diagnostics: true
  system_monitoring: true
  log_analysis: true
  rag_enabled: true

security:
  enable_auth: false
  api_key: null
  allowed_hosts: ["*"]
  rate_limit: 100

storage:
  models_dir: "/app/models"
  data_dir: "/app/data"
  logs_dir: "/app/logs"
  vector_db_path: "/app/data/chromadb"

qubinode:
  integration_enabled: true
  plugin_framework_path: "/opt/qubinode/core"
  ansible_callback: true
  setup_hooks: true

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "/app/logs/ai-assistant.log"
  max_size_mb: 100
  backup_count: 5
