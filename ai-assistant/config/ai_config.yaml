# Qubinode AI Assistant Configuration
# Based on ADR-0027: CPU-Based AI Deployment Assistant Architecture
#
# Default model: IBM Granite-4.0-Micro (3B parameters, CPU-optimized)
# This is a decoder-only dense transformer with GQA, RoPE, MLP with SwiGLU
# Supports 12 languages including English, German, Spanish, French, Japanese

ai_service:
  # Model configuration - can be overridden with environment variables
  # Default: granite-4.0-micro (IBM Granite 4.0 Micro, CPU-optimized per ADR-0027)
  model_type: "${AI_MODEL_TYPE:-granite-4.0-micro}"
  model_path: "${AI_MODEL_PATH:-/app/models/granite-4.0-micro.gguf}"
  model_url: "${AI_MODEL_URL:-https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q4_K_M.gguf}"

  # Hardware configuration
  use_gpu: "${AI_USE_GPU:-false}"
  gpu_layers: "${AI_GPU_LAYERS:-0}"  # Number of layers to offload to GPU
  threads: "${AI_THREADS:-4}"  # CPU threads to use

  # Server configuration
  llama_server_port: 8081
  context_length: "${AI_CONTEXT_LENGTH:-8192}"  # Granite 4.0 supports longer context
  temperature: "${AI_TEMPERATURE:-0.7}"
  max_tokens: "${AI_MAX_TOKENS:-512}"

  # Model presets for different hardware configurations
  model_presets:
    # Primary CPU-optimized model (ADR-0027 target)
    granite-4.0-micro:
      model_url: "https://huggingface.co/ibm-granite/granite-4.0-micro-GGUF/resolve/main/granite-4.0-micro-Q4_K_M.gguf"
      model_path: "/app/models/granite-4.0-micro.gguf"
      context_length: 8192
      parameters: "3B"
      recommended_for: "CPU-only, low memory (4GB+), ADR-0027 target model"

    # Alternative: bartowski quantization (may have different optimizations)
    granite-4.0-micro-bartowski:
      model_url: "https://huggingface.co/bartowski/ibm-granite_granite-4.0-micro-GGUF/resolve/main/ibm-granite_granite-4.0-micro-Q4_K_M.gguf"
      model_path: "/app/models/granite-4.0-micro-bartowski.gguf"
      context_length: 8192
      parameters: "3B"
      recommended_for: "Alternative quantization with potential optimizations"

    # Hybrid variant (may have different capabilities)
    granite-4.0-h-micro:
      model_url: "https://huggingface.co/ibm-granite/granite-4.0-h-micro-GGUF/resolve/main/granite-4.0-h-micro-Q4_K_M.gguf"
      model_path: "/app/models/granite-4.0-h-micro.gguf"
      context_length: 8192
      parameters: "3B"
      recommended_for: "CPU-only, hybrid architecture variant"

    # Legacy model (deprecated - kept for backwards compatibility)
    granite-3.0-2b:
      model_url: "https://huggingface.co/bartowski/granite-3.0-2b-instruct-GGUF/resolve/main/granite-3.0-2b-instruct-Q4_K_M.gguf"
      model_path: "/app/models/granite-3.0-2b.gguf"
      context_length: 4096
      parameters: "2B"
      recommended_for: "Legacy - superseded by granite-4.0-micro"
      deprecated: true

    # Larger model for systems with more resources
    granite-7b:
      model_url: "https://huggingface.co/instructlab/granite-7b-lab-GGUF/resolve/main/granite-7b-lab.Q4_K_M.gguf"
      model_path: "/app/models/granite-7b.gguf"
      context_length: 8192
      parameters: "7B"
      recommended_for: "CPU with 8GB+ RAM or GPU"

    # GPU-optimized models
    llama3-8b:
      model_url: "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
      model_path: "/app/models/llama3-8b.gguf"
      context_length: 8192
      parameters: "8B"
      gpu_layers: 32
      recommended_for: "GPU with 6GB+ VRAM"

    phi3-mini:
      model_url: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
      model_path: "/app/models/phi3-mini.gguf"
      context_length: 4096
      parameters: "3.8B"
      gpu_layers: 32
      recommended_for: "GPU with 4GB+ VRAM, fast inference"

    # LiteLLM-supported models (API-based)
    openai-gpt4:
      provider: "litellm"
      model_name: "gpt-4"
      api_endpoint: "https://api.openai.com/v1"
      recommended_for: "Cloud deployment with OpenAI API access"

    anthropic-claude:
      provider: "litellm"
      model_name: "claude-3-sonnet-20240229"
      api_endpoint: "https://api.anthropic.com"
      recommended_for: "Cloud deployment with Anthropic API access"

    azure-openai:
      provider: "litellm"
      model_name: "azure/gpt-4"
      api_endpoint: "${AZURE_API_BASE}"
      recommended_for: "Azure OpenAI deployment"

    ollama-local:
      provider: "litellm"
      model_name: "ollama/llama3"
      api_endpoint: "http://localhost:11434"
      recommended_for: "Local Ollama deployment with GPU"

server:
  host: "0.0.0.0"
  port: 8080
  log_level: "INFO"
  timeout: 30

features:
  diagnostics: true
  system_monitoring: true
  log_analysis: true
  rag_enabled: true

security:
  enable_auth: false
  api_key: null
  allowed_hosts: ["*"]
  rate_limit: 100

storage:
  models_dir: "/app/models"
  data_dir: "/app/data"
  logs_dir: "/app/logs"
  # Vector database path - Qdrant is the current implementation (ADR-0027 interim)
  # Target: PgVector migration per ADR-0049 (completed in Airflow, pending in AI Assistant)
  vector_db_path: "/app/data/qdrant-db"
  # Legacy paths for backwards compatibility:
  # - ChromaDB (deprecated): /app/data/vector-db
  # - FAISS (alternative): /app/data/faiss-db

qubinode:
  integration_enabled: true
  plugin_framework_path: "/opt/qubinode/core"
  ansible_callback: true
  setup_hooks: true

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "/app/logs/ai-assistant.log"
  max_size_mb: 100
  backup_count: 5
