"""
DAG Validator: Find, validate, and create DAGs for user requests.

This module provides:
- DAG discovery from Airflow dags folder
- DAG validation against user requirements
- DAG generation when no suitable DAG exists
- Pre-flight checks for domain-specific requirements

Per ADR-0066: Developer Agent DAG Validation and Smart Pipelines
Per ADR-0045: DAG Development Standards
"""

import re
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime

import httpx

logger = logging.getLogger(__name__)


class DAGStatus(str, Enum):
    """Status of a DAG in Airflow."""

    ACTIVE = "active"
    PAUSED = "paused"
    UNKNOWN = "unknown"
    NOT_FOUND = "not_found"


class ValidationStatus(str, Enum):
    """Validation result status."""

    PASSED = "passed"
    FAILED = "failed"
    WARNING = "warning"
    SKIPPED = "skipped"


@dataclass
class ValidationCheck:
    """Result of a single validation check."""

    name: str
    status: ValidationStatus
    message: str
    details: Optional[Dict[str, Any]] = None
    fix_suggestion: Optional[str] = None


@dataclass
class DAGInfo:
    """Information about an Airflow DAG."""

    dag_id: str
    file_path: str
    description: str = ""
    schedule: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    params: Dict[str, Any] = field(default_factory=dict)
    status: DAGStatus = DAGStatus.UNKNOWN
    is_suitable: bool = False
    match_score: float = 0.0
    validation_checks: List[ValidationCheck] = field(default_factory=list)


@dataclass
class DAGValidationResult:
    """Result of DAG validation."""

    dag_id: str
    is_valid: bool
    checks: List[ValidationCheck]
    overall_status: ValidationStatus
    can_proceed: bool
    user_actions: List[Dict[str, Any]] = field(default_factory=list)
    error_summary: Optional[str] = None


@dataclass
class DAGSearchResult:
    """Result of searching for DAGs."""

    found: bool
    matching_dags: List[DAGInfo]
    best_match: Optional[DAGInfo] = None
    create_new_recommended: bool = False
    recommendation: str = ""


@dataclass
class DAGCreationRequest:
    """Request to create a new DAG."""

    dag_id: str
    description: str
    task_description: str
    source_project: Optional[str] = None
    project_entry_point: Optional[str] = None
    params: Dict[str, Any] = field(default_factory=dict)
    similar_dags: List[str] = field(default_factory=list)


@dataclass
class DAGCreationResult:
    """Result of DAG creation."""

    success: bool
    dag_id: str
    file_path: Optional[str] = None
    dag_code: Optional[str] = None
    error: Optional[str] = None
    requires_approval: bool = True
    preview: Optional[str] = None


# DAG templates for common operations
DAG_TEMPLATES = {
    "vm_deployment": '''"""
Airflow DAG: {dag_id}
Auto-generated for: {description}

Per ADR-0045: DAG Development Standards
Per ADR-0066: Smart Pipeline Pattern
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

QUBINODE_PIPELINES_DIR = "{project_path}"

default_args = {{
    "owner": "qubinode",
    "depends_on_past": False,
    "start_date": datetime(2025, 1, 1),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=3),
}}

dag = DAG(
    "{dag_id}",
    default_args=default_args,
    description="""{description}""",
    schedule=None,
    catchup=False,
    tags=["qubinode", "auto-generated", "vm", "infrastructure"],
    params={{
        {params}
    }},
    doc_md="""
    # {dag_id}

    Auto-generated DAG for: {description}

    ## Parameters
    {param_docs}

    ## Generated by
    Developer Agent per ADR-0066
    """,
)

{tasks}

# Task dependencies
{dependencies}
''',
    "generic_deployment": '''"""
Airflow DAG: {dag_id}
Auto-generated for: {description}

Per ADR-0045: DAG Development Standards
Per ADR-0066: Smart Pipeline Pattern
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {{
    "owner": "qubinode",
    "depends_on_past": False,
    "start_date": datetime(2025, 1, 1),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=3),
}}

dag = DAG(
    "{dag_id}",
    default_args=default_args,
    description="""{description}""",
    schedule=None,
    catchup=False,
    tags=["qubinode", "auto-generated", "deployment"],
    params={{
        {params}
    }},
)

{tasks}

# Task dependencies
{dependencies}
''',
}


class DAGValidator:
    """
    Validates DAGs for user requests and creates new ones if needed.

    Provides:
    - DAG discovery from filesystem
    - Intent matching to find suitable DAGs
    - Validation of DAG parameters and prerequisites
    - DAG generation from templates
    """

    def __init__(
        self,
        dags_path: str = None,
        airflow_api_url: str = None,
    ):
        # Use environment variables or sensible defaults for container/host operation
        import os

        if dags_path is None:
            # Check if running in container (/app/airflow/dags) or on host
            dags_path = os.environ.get("AIRFLOW_DAGS_PATH", "/app/airflow/dags" if Path("/app/airflow/dags").exists() else "/opt/qubinode_navigator/airflow/dags")
        if airflow_api_url is None:
            airflow_api_url = os.environ.get("AIRFLOW_API_URL", "http://localhost:8888/api/v1")
        self.dags_path = Path(dags_path)
        self.airflow_api_url = airflow_api_url
        self._dag_cache: Dict[str, DAGInfo] = {}
        self._cache_timestamp: Optional[datetime] = None
        self._cache_ttl_seconds = 300  # 5 minutes

    async def discover_dags(self, force_refresh: bool = False) -> List[DAGInfo]:
        """
        Discover all DAGs from the filesystem.

        Parses DAG files to extract metadata for matching.
        """
        # Check cache
        if not force_refresh and self._cache_timestamp:
            age = (datetime.now() - self._cache_timestamp).total_seconds()
            if age < self._cache_ttl_seconds and self._dag_cache:
                return list(self._dag_cache.values())

        dags: List[DAGInfo] = []

        if not self.dags_path.exists():
            logger.warning(f"DAGs path does not exist: {self.dags_path}")
            return dags

        # Scan for Python files
        for dag_file in self.dags_path.glob("*.py"):
            # Skip helper/utility files
            if dag_file.name.startswith("_") or dag_file.name in [
                "dag_factory.py",
                "dag_helpers.py",
                "dag_loader.py",
                "dag_logging_mixin.py",
            ]:
                continue

            try:
                dag_info = await self._parse_dag_file(dag_file)
                if dag_info:
                    dags.append(dag_info)
                    self._dag_cache[dag_info.dag_id] = dag_info
            except Exception as e:
                logger.warning(f"Failed to parse {dag_file}: {e}")

        self._cache_timestamp = datetime.now()
        logger.info(f"Discovered {len(dags)} DAGs")
        return dags

    async def _parse_dag_file(self, file_path: Path) -> Optional[DAGInfo]:
        """Parse a DAG file to extract metadata."""
        content = file_path.read_text()

        # Extract DAG ID
        dag_id_match = re.search(r'DAG\s*\(\s*["\']([^"\']+)["\']', content)
        if not dag_id_match:
            # Try alternate patterns
            dag_id_match = re.search(r'dag_id\s*=\s*["\']([^"\']+)["\']', content)

        if not dag_id_match:
            return None

        dag_id = dag_id_match.group(1)

        # Extract description
        desc_match = re.search(r'description\s*=\s*["\']([^"\']+)["\']', content)
        description = desc_match.group(1) if desc_match else ""

        # Extract schedule
        schedule_match = re.search(r'schedule\s*=\s*["\']?([^"\']+)["\']?', content)
        schedule = schedule_match.group(1) if schedule_match else None
        if schedule == "None":
            schedule = None

        # Extract tags
        tags_match = re.search(r"tags\s*=\s*\[([^\]]+)\]", content)
        tags = []
        if tags_match:
            tags_str = tags_match.group(1)
            tags = re.findall(r'["\']([^"\']+)["\']', tags_str)

        # Extract params
        params = {}
        params_match = re.search(r"params\s*=\s*\{([^}]+)\}", content, re.DOTALL)
        if params_match:
            params_str = params_match.group(1)
            # Parse simple param definitions
            for param_match in re.finditer(r'["\'](\w+)["\']\s*:\s*([^,\n]+)', params_str):
                param_name = param_match.group(1)
                param_value = param_match.group(2).strip().strip("\"'")
                params[param_name] = param_value

        return DAGInfo(
            dag_id=dag_id,
            file_path=str(file_path),
            description=description,
            schedule=schedule,
            tags=tags,
            params=params,
        )

    async def find_dag_for_task(
        self,
        task_description: str,
    ) -> DAGSearchResult:
        """
        Find DAGs that can fulfill a task.

        Uses keyword matching and tag analysis to find suitable DAGs.
        """
        dags = await self.discover_dags()

        task_lower = task_description.lower()
        matching_dags: List[DAGInfo] = []

        # Keywords to DAG mapping patterns
        keyword_patterns = {
            "vm": ["vm", "virtual machine", "centos", "rhel", "fedora", "ubuntu"],
            "openshift": ["openshift", "ocp", "cluster", "kubernetes", "k8s"],
            "freeipa": ["freeipa", "identity", "ldap", "kerberos", "dns"],
            "harbor": ["harbor", "container registry", "docker registry"],
            "registry": ["registry", "mirror", "quay"],
            "vyos": ["vyos", "router", "firewall", "network"],
            "step_ca": ["step ca", "certificate", "pki", "tls"],
            "vault": ["vault", "secrets", "hashicorp"],
        }

        for dag in dags:
            score = 0.0

            # Check description match
            if dag.description:
                desc_lower = dag.description.lower()
                for word in task_lower.split():
                    if len(word) > 3 and word in desc_lower:
                        score += 0.1

            # Check tag match
            for tag in dag.tags:
                if tag.lower() in task_lower:
                    score += 0.15

            # Check keyword patterns
            for pattern_type, keywords in keyword_patterns.items():
                task_matches = any(kw in task_lower for kw in keywords)
                dag_matches = pattern_type in dag.dag_id.lower() or any(pattern_type in tag.lower() for tag in dag.tags) or pattern_type in dag.description.lower()
                if task_matches and dag_matches:
                    score += 0.3

            # Check specific actions
            if "create" in task_lower and "deployment" in dag.dag_id:
                score += 0.1
            if "delete" in task_lower and "deployment" in dag.dag_id:
                score += 0.1
            if "deploy" in task_lower and "deployment" in dag.dag_id:
                score += 0.2

            if score > 0:
                dag.match_score = min(score, 1.0)
                dag.is_suitable = score >= 0.3
                matching_dags.append(dag)

        # Sort by match score
        matching_dags.sort(key=lambda d: d.match_score, reverse=True)

        # Determine best match
        best_match = matching_dags[0] if matching_dags else None

        # Decide if we should recommend creating a new DAG
        create_new = not matching_dags or (best_match and best_match.match_score < 0.5)

        recommendation = ""
        if not matching_dags:
            recommendation = f"No existing DAGs match '{task_description}'. A new DAG should be created."
        elif best_match and best_match.match_score >= 0.7:
            recommendation = f"DAG '{best_match.dag_id}' is a strong match for this task."
        elif best_match and best_match.match_score >= 0.5:
            recommendation = f"DAG '{best_match.dag_id}' may work. Consider validating parameters."
        else:
            recommendation = "No strong matches found. Consider creating a custom DAG."

        return DAGSearchResult(
            found=bool(matching_dags),
            matching_dags=matching_dags[:5],  # Top 5
            best_match=best_match,
            create_new_recommended=create_new,
            recommendation=recommendation,
        )

    async def validate_dag(
        self,
        dag_id: str,
        task_params: Dict[str, Any],
    ) -> DAGValidationResult:
        """
        Validate a DAG can fulfill the task requirements.

        Checks:
        - DAG exists and is not paused
        - Required parameters are provided
        - Parameter values are valid (templates exist, etc.)
        - No blocking dependencies
        """
        checks: List[ValidationCheck] = []

        # Check 1: DAG exists
        dag_info = self._dag_cache.get(dag_id)
        if not dag_info:
            # Refresh cache and try again
            await self.discover_dags(force_refresh=True)
            dag_info = self._dag_cache.get(dag_id)

        if not dag_info:
            checks.append(
                ValidationCheck(
                    name="dag_exists",
                    status=ValidationStatus.FAILED,
                    message=f"DAG '{dag_id}' not found",
                    fix_suggestion="Create DAG or use a different DAG ID",
                )
            )
            return DAGValidationResult(
                dag_id=dag_id,
                is_valid=False,
                checks=checks,
                overall_status=ValidationStatus.FAILED,
                can_proceed=False,
                error_summary=f"DAG '{dag_id}' does not exist",
            )

        checks.append(
            ValidationCheck(
                name="dag_exists",
                status=ValidationStatus.PASSED,
                message=f"DAG '{dag_id}' exists at {dag_info.file_path}",
            )
        )

        # Check 2: DAG status in Airflow
        dag_status = await self._check_dag_status(dag_id)
        if dag_status == DAGStatus.PAUSED:
            checks.append(
                ValidationCheck(
                    name="dag_active",
                    status=ValidationStatus.WARNING,
                    message=f"DAG '{dag_id}' is paused",
                    fix_suggestion="Unpause the DAG in Airflow UI or via CLI",
                )
            )
        elif dag_status == DAGStatus.ACTIVE:
            checks.append(
                ValidationCheck(
                    name="dag_active",
                    status=ValidationStatus.PASSED,
                    message=f"DAG '{dag_id}' is active",
                )
            )
        else:
            checks.append(
                ValidationCheck(
                    name="dag_active",
                    status=ValidationStatus.SKIPPED,
                    message="Could not verify DAG status (Airflow API unavailable)",
                )
            )

        # Check 3: Required parameters
        missing_params = []
        for param_name in dag_info.params.keys():
            if param_name not in task_params:
                # Check if it has a default
                default_value = dag_info.params.get(param_name)
                if default_value in (None, "", "None"):
                    missing_params.append(param_name)

        if missing_params:
            checks.append(
                ValidationCheck(
                    name="required_params",
                    status=ValidationStatus.WARNING,
                    message=f"Optional parameters not provided: {missing_params}",
                    details={"missing": missing_params},
                    fix_suggestion=f"Provide values for: {', '.join(missing_params)}",
                )
            )
        else:
            checks.append(
                ValidationCheck(
                    name="required_params",
                    status=ValidationStatus.PASSED,
                    message="All required parameters available",
                )
            )

        # Check 4: Domain-specific validation
        domain_checks = await self._run_domain_checks(dag_id, task_params)
        checks.extend(domain_checks)

        # Calculate overall status
        failed_count = sum(1 for c in checks if c.status == ValidationStatus.FAILED)
        warning_count = sum(1 for c in checks if c.status == ValidationStatus.WARNING)

        if failed_count > 0:
            overall_status = ValidationStatus.FAILED
            can_proceed = False
        elif warning_count > 0:
            overall_status = ValidationStatus.WARNING
            can_proceed = True  # Can proceed with warnings
        else:
            overall_status = ValidationStatus.PASSED
            can_proceed = True

        # Build user actions
        user_actions = []
        for check in checks:
            if check.status == ValidationStatus.FAILED and check.fix_suggestion:
                user_actions.append(
                    {
                        "action_type": "self_fix",
                        "check_name": check.name,
                        "description": check.fix_suggestion,
                        "difficulty": "moderate",
                    }
                )

        if not can_proceed:
            user_actions.append(
                {
                    "action_type": "escalate",
                    "description": "Get help from Manager Agent",
                    "api_endpoint": "/orchestrator/escalate",
                    "difficulty": "easy",
                }
            )

        # Error summary
        error_summary = None
        if failed_count > 0:
            failed_checks = [c for c in checks if c.status == ValidationStatus.FAILED]
            error_summary = "; ".join(c.message for c in failed_checks)

        return DAGValidationResult(
            dag_id=dag_id,
            is_valid=(overall_status != ValidationStatus.FAILED),
            checks=checks,
            overall_status=overall_status,
            can_proceed=can_proceed,
            user_actions=user_actions,
            error_summary=error_summary,
        )

    async def _check_dag_status(self, dag_id: str) -> DAGStatus:
        """Check DAG status via Airflow API."""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(
                    f"{self.airflow_api_url}/dags/{dag_id}",
                    auth=("admin", "admin"),  # Default airflow credentials
                )
                if response.status_code == 200:
                    data = response.json()
                    if data.get("is_paused"):
                        return DAGStatus.PAUSED
                    return DAGStatus.ACTIVE
                elif response.status_code == 404:
                    return DAGStatus.NOT_FOUND
        except Exception as e:
            logger.debug(f"Could not check DAG status: {e}")
        return DAGStatus.UNKNOWN

    async def _run_domain_checks(
        self,
        dag_id: str,
        params: Dict[str, Any],
    ) -> List[ValidationCheck]:
        """Run domain-specific validation checks."""
        checks = []
        dag_id_lower = dag_id.lower()

        # VM deployment checks
        if "vm" in dag_id_lower or "deployment" in dag_id_lower:
            checks.extend(await self._vm_domain_checks(params))

        # OpenShift checks
        if "openshift" in dag_id_lower or "ocp" in dag_id_lower:
            checks.extend(await self._openshift_domain_checks(params))

        # Registry checks
        if "registry" in dag_id_lower or "harbor" in dag_id_lower:
            checks.extend(await self._registry_domain_checks(params))

        return checks

    async def _vm_domain_checks(
        self,
        params: Dict[str, Any],
    ) -> List[ValidationCheck]:
        """VM-specific validation checks."""
        checks = []

        # Check if vm_profile/template exists
        vm_profile = params.get("vm_profile") or params.get("template")
        if vm_profile:
            # This would check kcli templates - simplified for now
            checks.append(
                ValidationCheck(
                    name="vm_profile_check",
                    status=ValidationStatus.PASSED,
                    message=f"VM profile '{vm_profile}' will be used",
                    details={"profile": vm_profile},
                )
            )

        return checks

    async def _openshift_domain_checks(
        self,
        params: Dict[str, Any],
    ) -> List[ValidationCheck]:
        """OpenShift-specific validation checks."""
        checks = []

        # Check pull secret
        pull_secret = params.get("pull_secret")
        if not pull_secret:
            checks.append(
                ValidationCheck(
                    name="pull_secret_check",
                    status=ValidationStatus.WARNING,
                    message="Pull secret not provided - will use default if available",
                    fix_suggestion="Provide OpenShift pull secret from cloud.redhat.com",
                )
            )
        else:
            checks.append(
                ValidationCheck(
                    name="pull_secret_check",
                    status=ValidationStatus.PASSED,
                    message="Pull secret provided",
                )
            )

        return checks

    async def _registry_domain_checks(
        self,
        params: Dict[str, Any],
    ) -> List[ValidationCheck]:
        """Registry-specific validation checks."""
        checks = []

        # Check storage
        storage_size = params.get("storage_size")
        if storage_size:
            checks.append(
                ValidationCheck(
                    name="storage_check",
                    status=ValidationStatus.PASSED,
                    message=f"Storage size: {storage_size}",
                )
            )

        return checks

    async def generate_dag(
        self,
        request: DAGCreationRequest,
    ) -> DAGCreationResult:
        """
        Generate a new DAG based on the request.

        Uses templates and similar DAGs as reference.
        """
        # Determine template to use
        template_name = "generic_deployment"
        if "vm" in request.task_description.lower():
            template_name = "vm_deployment"

        template = DAG_TEMPLATES.get(template_name, DAG_TEMPLATES["generic_deployment"])

        # Build params string
        params_list = []
        param_docs = []
        for key, value in request.params.items():
            if isinstance(value, str):
                params_list.append(f'        "{key}": "{value}",')
            else:
                params_list.append(f'        "{key}": {value},')
            param_docs.append(f"- **{key}**: {value}")

        params_str = "\n".join(params_list) if params_list else "        # No params defined"
        param_docs_str = "\n    ".join(param_docs) if param_docs else "No parameters"

        # Determine project path
        project_path = "/opt/qubinode-pipelines"
        if request.source_project:
            from project_registry import get_project_registry

            try:
                registry = await get_project_registry()
                project = registry.get_project(request.source_project)
                if project:
                    project_path = project.path
            except Exception:
                pass

        # Build tasks - simple initial task
        tasks = '''
# Task: Execute deployment
execute_deployment = BashOperator(
    task_id="execute_deployment",
    bash_command="""
    export PATH="/home/airflow/.local/bin:/usr/local/bin:$PATH"
    echo "========================================"
    echo "Executing: {description}"
    echo "========================================"

    # Add deployment logic here
    echo "[INFO] Deployment task started"

    echo "[OK] Deployment complete"
    """,
    dag=dag,
)
'''
        tasks = tasks.format(description=request.description)

        dependencies = "# No dependencies for single task"

        # Generate DAG code
        dag_code = template.format(
            dag_id=request.dag_id,
            description=request.description,
            project_path=project_path,
            params=params_str,
            param_docs=param_docs_str,
            tasks=tasks,
            dependencies=dependencies,
        )

        # Determine file path
        file_path = self.dags_path / f"{request.dag_id}.py"

        # Create preview (first 50 lines)
        preview_lines = dag_code.split("\n")[:50]
        preview = "\n".join(preview_lines)
        if len(dag_code.split("\n")) > 50:
            preview += "\n# ... (truncated)"

        return DAGCreationResult(
            success=True,
            dag_id=request.dag_id,
            file_path=str(file_path),
            dag_code=dag_code,
            requires_approval=True,
            preview=preview,
        )

    async def deploy_dag(
        self,
        dag_code: str,
        file_path: str,
    ) -> Tuple[bool, str]:
        """
        Deploy a DAG to the filesystem.

        Returns (success, message).
        """
        try:
            path = Path(file_path)
            path.write_text(dag_code)
            return True, f"DAG deployed to {file_path}"
        except Exception as e:
            return False, f"Failed to deploy DAG: {e}"


# Singleton instance
_validator: Optional[DAGValidator] = None


async def get_dag_validator() -> DAGValidator:
    """Get the singleton DAG validator instance."""
    global _validator
    if _validator is None:
        _validator = DAGValidator()
    return _validator


async def find_or_create_dag(
    task_description: str,
    params: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Convenience function to find or create a DAG for a task.

    Returns dict with:
    - found: Whether an existing DAG was found
    - dag_id: The DAG ID (existing or proposed)
    - validation: Validation result if existing DAG
    - creation: Creation result if new DAG needed
    - user_actions: Actions the user can take
    """
    validator = await get_dag_validator()
    params = params or {}

    # First, search for existing DAGs
    search_result = await validator.find_dag_for_task(task_description)

    if search_result.best_match and search_result.best_match.match_score >= 0.5:
        # Validate the best match
        validation = await validator.validate_dag(
            search_result.best_match.dag_id,
            params,
        )

        return {
            "found": True,
            "dag_id": search_result.best_match.dag_id,
            "match_score": search_result.best_match.match_score,
            "validation": {
                "is_valid": validation.is_valid,
                "can_proceed": validation.can_proceed,
                "overall_status": validation.overall_status.value,
                "checks": [
                    {
                        "name": c.name,
                        "status": c.status.value,
                        "message": c.message,
                    }
                    for c in validation.checks
                ],
                "error_summary": validation.error_summary,
            },
            "user_actions": validation.user_actions,
            "recommendation": search_result.recommendation,
        }

    # No suitable DAG found - prepare creation
    # Generate a DAG ID from the task description
    dag_id = re.sub(r"[^a-z0-9]+", "_", task_description.lower())[:50]
    dag_id = dag_id.strip("_")
    if not dag_id:
        dag_id = "custom_deployment"

    creation_request = DAGCreationRequest(
        dag_id=dag_id,
        description=task_description,
        task_description=task_description,
        params=params,
        similar_dags=[d.dag_id for d in search_result.matching_dags[:3]],
    )

    creation_result = await validator.generate_dag(creation_request)

    return {
        "found": False,
        "dag_id": dag_id,
        "create_recommended": True,
        "creation": {
            "success": creation_result.success,
            "file_path": creation_result.file_path,
            "preview": creation_result.preview,
            "requires_approval": creation_result.requires_approval,
        },
        "similar_dags": [{"dag_id": d.dag_id, "score": d.match_score} for d in search_result.matching_dags[:3]],
        "user_actions": [
            {
                "action_type": "approve",
                "description": "Approve and deploy the generated DAG",
                "api_endpoint": "/orchestrator/dag/approve",
            },
            {
                "action_type": "modify",
                "description": "Request modifications to the DAG",
                "api_endpoint": "/orchestrator/dag/modify",
            },
            {
                "action_type": "reject",
                "description": "Reject and try alternatives",
                "api_endpoint": "/orchestrator/dag/reject",
            },
        ],
        "recommendation": search_result.recommendation,
    }
